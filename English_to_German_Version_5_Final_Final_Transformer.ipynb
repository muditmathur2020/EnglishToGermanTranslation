{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muditmathur2020/EnglishToGermanTranslation/blob/main/English_to_German_Version_5_Final_Final_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgriPT0wETsq"
      },
      "source": [
        "### mount google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzcb1Hu3C9Sh",
        "outputId": "b5810ee2-84da-4883-9984-d05641e2314d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBnbCu6GHbMJ"
      },
      "source": [
        "# Step 1: Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjF371EVHM6n"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time # to see how long it takes in training\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import warnings\n",
        "from wordcloud import WordCloud\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas()\n",
        "from tensorflow.keras import backend as K\n",
        "logger = tf.get_logger()\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Input, TimeDistributed, Concatenate, RepeatVector, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.models import Model, load_model, Sequential\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import string\n",
        "import re\n",
        "from unicodedata import normalize\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import LSTM,Dense,Embedding,RepeatVector,TimeDistributed\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import pandas as pd\n",
        "from string import punctuation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from string import digits\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwAD6AfAJSWp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64550624-1282-4c48-fdf8-0d8b5a8d6623"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds # tools for the tokenizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovVn780GHrrc"
      },
      "source": [
        "# Step 2: Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiC7Z0ACNWjd"
      },
      "source": [
        "### read files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "My08LADJHrFo",
        "outputId": "9d969bcd-1a15-476c-f57c-b53bc570bf49"
      },
      "source": [
        "\n",
        "europarl_en = open('/content/drive/MyDrive/Capstone /europarl-v7_en_de.txt', encoding='utf-8')\n",
        "europarl_de = open('/content/drive/MyDrive/Capstone /europarl-v7_de_en.txt', encoding='utf-8')\n",
        "print(\"Loaded europarl dataset\")\n",
        "\n",
        "# read the content\n",
        "text_en = pd.Series(europarl_en.readlines())\n",
        "text_de = pd.Series(europarl_de.readlines())\n",
        "\n",
        "print(text_en[:50])\n",
        "print(text_de[:50])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded europarl dataset\n",
            "0                           Resumption of the session\\n\n",
            "1     I declare resumed the session of the European ...\n",
            "2     Although, as you will have seen, the dreaded '...\n",
            "3     You have requested a debate on this subject in...\n",
            "4     In the meantime, I should like to observe a mi...\n",
            "5      Please rise, then, for this minute' s silence.\\n\n",
            "6     (The House rose and observed a minute' s silen...\n",
            "7               Madam President, on a point of order.\\n\n",
            "8     You will be aware from the press and televisio...\n",
            "9     One of the people assassinated very recently i...\n",
            "10    Would it be appropriate for you, Madam Preside...\n",
            "11    Yes, Mr Evans, I feel an initiative of the typ...\n",
            "12    If the House agrees, I shall do as Mr Evans ha...\n",
            "13              Madam President, on a point of order.\\n\n",
            "14    I would like your advice about Rule 143 concer...\n",
            "15    My question relates to something that will com...\n",
            "16    The Cunha report on multiannual guidance progr...\n",
            "17    It says that this should be done despite the p...\n",
            "18    I believe that the principle of relative stabi...\n",
            "19    I want to know whether one can raise an object...\n",
            "20    That is precisely the time when you may, if yo...\n",
            "21                                                   \\n\n",
            "22    Madam President, coinciding with this year' s ...\n",
            "23    At the request of a French Member, Mr Zimeray,...\n",
            "24    However, I would ask you, in accordance with t...\n",
            "25    This is all in accordance with the principles ...\n",
            "26         Thank you, Mr Segni, I shall do so gladly.\\n\n",
            "27    Indeed, it is quite in keeping with the positi...\n",
            "28    Madam President, I should like to draw your at...\n",
            "29               It is the case of Alexander Nikitin.\\n\n",
            "30    All of us here are pleased that the courts hav...\n",
            "31    Now, however, he is to go before the courts on...\n",
            "32    We know, and we have stated as much in very ma...\n",
            "33    These findings form the basis of the European ...\n",
            "34    Yes, Mrs Schroedter, I shall be pleased to loo...\n",
            "35    Madam President, I would firstly like to compl...\n",
            "36    But, Madam President, my personal request has ...\n",
            "37    Although there are now two Finnish channels an...\n",
            "38    I would therefore once more ask you to ensure ...\n",
            "39    Mrs Plooij-van Gorsel, I can tell you that thi...\n",
            "40    It will, I hope, be examined in a positive lig...\n",
            "41    Madam President, can you tell me why this Parl...\n",
            "42    Why has no air quality test been done on this ...\n",
            "43    Why has there been no Health and Safety Commit...\n",
            "44    Why has there been no fire drill, either in th...\n",
            "45                Why are there no fire instructions?\\n\n",
            "46    Why have the staircases not been improved sinc...\n",
            "47             Why are no-smoking areas not enforced?\\n\n",
            "48    It seems absolutely disgraceful that we pass l...\n",
            "49    Mrs Lynne, you are quite right and I shall che...\n",
            "dtype: object\n",
            "0                  Wiederaufnahme der Sitzungsperiode\\n\n",
            "1     Ich erkläre die am Freitag, dem 17. Dezember u...\n",
            "2     Wie Sie feststellen konnten, ist der gefürchte...\n",
            "3     Im Parlament besteht der Wunsch nach einer Aus...\n",
            "4     Heute möchte ich Sie bitten - das ist auch der...\n",
            "5     Ich bitte Sie, sich zu einer Schweigeminute zu...\n",
            "6     (Das Parlament erhebt sich zu einer Schweigemi...\n",
            "7             Frau Präsidentin, zur Geschäftsordnung.\\n\n",
            "8     Wie Sie sicher aus der Presse und dem Fernsehe...\n",
            "9     Zu den Attentatsopfern, die es in jüngster Zei...\n",
            "10    Wäre es angemessen, wenn Sie, Frau Präsidentin...\n",
            "11    Ja, Herr Evans, ich denke, daß eine derartige ...\n",
            "12    Wenn das Haus damit einverstanden ist, werde i...\n",
            "13            Frau Präsidentin, zur Geschäftsordnung.\\n\n",
            "14    Könnten Sie mir eine Auskunft zu Artikel 143 i...\n",
            "15    Meine Frage betrifft eine Angelegenheit, die a...\n",
            "16    Das Parlament wird sich am Donnerstag mit dem ...\n",
            "17    Und zwar sollen derartige Strafen trotz des Gr...\n",
            "18    Ich meine, daß der Grundsatz der relativen Sta...\n",
            "19    Ich möchte wissen, ob es möglich ist, einen Ei...\n",
            "20    Genau dann können Sie, wenn Sie wollen, diese ...\n",
            "21                                  Frau Präsidentin!\\n\n",
            "22    Die erste diesjährige Tagung des Europäischen ...\n",
            "23    Auf Wunsch eines französischen Mitglieds, Herr...\n",
            "24    Gemäß der vom Europäischen Parlament und von d...\n",
            "25    All dies entspricht den Grundsätzen, die wir s...\n",
            "26    Vielen Dank, Herr Segni, das will ich gerne tu...\n",
            "27    Das ist ganz im Sinne der Position, die wir al...\n",
            "28    Frau Präsidentin! Ich möchte Sie auf einen Fal...\n",
            "29            Das ist der Fall von Alexander Nikitin.\\n\n",
            "30    Wir freuen uns hier alle, daß das Gericht ihn ...\n",
            "31    Nun ist es aber so, daß er wieder angeklagt we...\n",
            "32    Wir wissen und wir haben es in wirklich sehr v...\n",
            "33    Diese Ergebnisse sind die Grundlage für die eu...\n",
            "34    Frau Schroedter, ich bin gerne bereit, die dam...\n",
            "35    Frau Präsidentin, zunächst besten Dank dafür, ...\n",
            "36    Dennoch, Frau Präsidentin, wurde meinem Wunsch...\n",
            "37    Zwar können wir jetzt zwei finnische und einen...\n",
            "38    Deshalb möchte ich Sie nochmals ersuchen, dafü...\n",
            "39    Frau Plooij-van Gorsel, ich kann Ihnen mitteil...\n",
            "40    Ich hoffe, daß dort in Ihrem Sinne entschieden...\n",
            "41    Frau Präsidentin, können Sie mir sagen, warum ...\n",
            "42    Weshalb wurde die Luftqualität in diesem Gebäu...\n",
            "43    Weshalb ist der Arbeitsschutzausschuß seit 199...\n",
            "44    Warum hat weder im Brüsseler noch im Straßburg...\n",
            "45    Warum finden keine Brandschutzbelehrungen stat...\n",
            "46    Warum wurde nach meinem Unfall nichts unternom...\n",
            "47    Warum wird in den Nichtraucherzonen das Rauchv...\n",
            "48    Es ist eine Schande, daß wir Regeln verabschie...\n",
            "49    Frau Lynne, Sie haben völlig recht, und ich we...\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqSKydTgW4nE"
      },
      "source": [
        "# with open(\"data/nonbreaking_prefix.en\", mode='r', encoding=\"utf-8\") as f:\n",
        "#     non_breaking_prefix_en = f.read()\n",
        "\n",
        "# with open(\"data/nonbreaking_prefix.de\", mode='r', encoding=\"utf-8\") as f:\n",
        "#     non_breaking_prefix_de = f.read()\n",
        "# print(non_breaking_prefix_en[:5])\n",
        "# print(non_breaking_prefix_de[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mFVDgeZOzrU"
      },
      "source": [
        "### Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Skngci4mOOiO"
      },
      "source": [
        "# non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
        "# non_breaking_prefix_en = [' ' + pref.lower() + '.' for pref in non_breaking_prefix_en]\n",
        "# non_breaking_prefix_de = non_breaking_prefix_de.split(\"\\n\")\n",
        "# non_breaking_prefix_de = [' ' + pref.lower() + '.' for pref in non_breaking_prefix_de]\n",
        "# print(non_breaking_prefix_en[:5])\n",
        "# print(non_breaking_prefix_de[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgcNPbA8SE9b"
      },
      "source": [
        "# for prefix in non_breaking_prefix_en:\n",
        "#     text_en = text_en.replace(prefix, prefix + '###')\n",
        "# text_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\", text_en)\n",
        "# text_en = re.sub(r\"\\.###\", '', text_en)\n",
        "# text_en = re.sub(r\" +\", ' ', text_en)\n",
        "# text_en = text_en.replace('###', ' ')\n",
        "\n",
        "# text_en = text_en.split(\"\\n\")\n",
        "\n",
        "# for prefix in non_breaking_prefix_de:\n",
        "#     text_de = text_de.replace(prefix, prefix + '###')\n",
        "# text_de = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\", text_de)\n",
        "# text_de = re.sub(r\"\\.###\", '', text_de)\n",
        "# text_de = re.sub(r\" +\", ' ', text_de)\n",
        "# text_de = text_de.replace('###', ' ')\n",
        "\n",
        "# text_de = text_de.split(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R96V-csIdC1p"
      },
      "source": [
        "### Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzNjGhLKT8OR"
      },
      "source": [
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    text_en, target_vocab_size=8000)\n",
        "tokenizer_de = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    text_de, target_vocab_size=8000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z5zOdOEgvVm"
      },
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2\n",
        "VOCAB_SIZE_DE = tokenizer_de.vocab_size + 2\n",
        "\n",
        "# we put start and end tokens as size-1 and size-2 which are the same as\n",
        "# tokenizer_size and tokenizer_size+1 because the words are from [0 to ts-1]\n",
        "# tokenize_en.encode(sentence) give a list then list + list + list appends them\n",
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
        "          for sentence in text_en]\n",
        "outputs = [[VOCAB_SIZE_DE-2] + tokenizer_de.encode(sentence) + [VOCAB_SIZE_DE-1]\n",
        "          for sentence in text_de]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HONDBNqJsIqU"
      },
      "source": [
        "### Remove too long sentences\n",
        "Why?\n",
        "(1) because when we pad we will have a hugeeee ram issuie for example sentence sizes of 1,100,2 when we pad they become 100,100,100 which we would rather loose that 100 than pad all to 100\n",
        "(2) takes too much time to train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRoMWeU7r-tD"
      },
      "source": [
        "MAX_LENGTH = 20 # we will still have a lot of data with max len of 20\n",
        "\n",
        "# this part, why we do it is a bit tricky, pay attention why we do it like this:\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "# we remove in reversed because of shifting issuies when we start from begining\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "\n",
        "# same stuff for outputs>20\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wvxrND85t7r"
      },
      "source": [
        "### input/output creation\n",
        "1) padding  \n",
        "2) batching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47hjlJ1SvXwK"
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_LENGTH)\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVq4wN816gOj"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000 # how much data to keep\n",
        "\n",
        "# now we turned our data into a dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "\n",
        "# this is something that improves the way the dataset is stored, it increases\n",
        "# the speed of accessing the data which increases training speed in return:\n",
        "dataset = dataset.cache()\n",
        "\n",
        "# now we shuffle in batches\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "# this increases the speed even further:\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoOMHhKU8kFo"
      },
      "source": [
        "# Step 3: Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alCnJ2ZtTNJt"
      },
      "source": [
        "## A - Positional Encoding (look at the formula in the paper)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoCaYr408as-"
      },
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        # this Positional Encoder we made it a child of the Layers so it has all\n",
        "        # the properties that a layer has\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        \"\"\"\n",
        "        :pos: (seq_len, 1) index of the word in sentence [0 to 19]\n",
        "        :i: the dimensions of the embedding (glove dims 200) then-> [0 to 199]\n",
        "        :d_model: the size (dimension) of the embeded (e.g. glove size 200)\n",
        "        :return: (seq_len, d_model) why? we are getting the encoding of the\n",
        "                every positions vs every one of the dimensions of that word\n",
        "        \"\"\"\n",
        "        angles = 1 / np.power(10000., (2*(i//2))/np.float32(d_model))\n",
        "        return pos * angles # dim: (seq_len, d_model)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # input.shape = [batch_size, multihead_size(sz=8), each word (pos), that words embedding]\n",
        "        # keep in mind we DONT change the values of the input considering\n",
        "        # their positions, we just get the dims from input and calculate\n",
        "        # pos encoding totally seperatly and stack them at the end\n",
        "        seq_length = inputs.shape.as_list()[-2] # basically the pos\n",
        "        d_model = inputs.shape.as_list()[-1] # basically the embedded values\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                                 np.arange(d_model)[np.newaxis, :],\n",
        "                                 d_model)\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        # we do this because it has a [batch] dimension at the begining we add\n",
        "        # it. why? because inputs and the encodings need to be same dims so we\n",
        "        # make newaxis which it doesnt put 0's.... it copies those same dims for\n",
        "        # all the batches...\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        # now we need to return both the inputs and their pos_encodings\n",
        "        # but we have pos_encoding in np so we make them tf\n",
        "        return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BEvzpOwa-RD"
      },
      "source": [
        "## B - Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-8AbGEHbFua"
      },
      "source": [
        "### Attention computation (see the formula in the paper)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4SX3F__VjXc"
      },
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "    # Q*K will be [output_len, d_model] * [d_model, input_len] which both are 20\n",
        "    # for both english and french\n",
        "    # the transpose_b=True makes keys turn to keys.T\n",
        "    # each of them are this dim: [batch_size, nb_proj, seq_len, d_proj]\n",
        "    # so with transpose it become: [a,b,c,d] * []\n",
        "    product = tf.matmul(queries, keys, transpose_b=True)\n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32) # makes the dim_num float\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim) # scales it (formula stuff)\n",
        "\n",
        "    # because this mask as the paper said is optional to prevent the program\n",
        "    # from seeing the feauture. why? because when we backprop then they will\n",
        "    # consider the stuff in front of them so to stop this we add -1e9 to them\n",
        "    # so after softmax the probabilities become 0 for them\n",
        "    if mask is not None:\n",
        "        scaled_product += (mask * -1e9)\n",
        "\n",
        "    # we apply the softmax along the last axis because we want their sum to be 1\n",
        "    # scaled_product = [output_len, input_len] -> softmax on input_len so\n",
        "    # basically we are keeping in_len the same but finding the probs for out_len\n",
        "    # so for every ins what is the prob of each of the outs\n",
        "    # (e.g. ith input, the probs [0.3,0.7] of the outs)\n",
        "    probs = tf.nn.softmax(scaled_product, axis=-1)\n",
        "\n",
        "    # attention = [output_len, input_len] * [input_len, d_model] = [output_len, d_model]\n",
        "    # so now we have d_model weights for each of the output words which we will\n",
        "    # feed to forwards to see their prediction for each of the out_lens\n",
        "    attention = tf.matmul(probs, values)\n",
        "\n",
        "    return attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7xS01sz8Yzn"
      },
      "source": [
        "# import numpy as np\n",
        "# # this is just a test for you to see what happens in this line of code to the\n",
        "# # dims (which we realized from the 4 dims, it only transposed the last two and\n",
        "# # did mult only on those last two because matmul considers the other dims as\n",
        "# # batch size and other stuff) (tf.matmul(a, b, transpose_b=True))\n",
        "# a = np.arange(24).reshape(1,2,3,4)\n",
        "# a = tf.convert_to_tensor(a, np.float32)\n",
        "# b = np.arange(24).reshape(1,2,3,4)\n",
        "# b = tf.convert_to_tensor(b, np.float32)\n",
        "# product = tf.matmul(a, b, transpose_b=True)\n",
        "# print(product.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTfOzkdMbK2j"
      },
      "source": [
        "### Multi-Head attention sublayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzN_spFEbS5K"
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    def __init__(self, nb_proj):\n",
        "        \"\"\"\n",
        "        :nb_proj: the number of projections for the multihead\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.nb_proj = nb_proj\n",
        "\n",
        "    # this is the same as init but it happens when we USE the object for the\n",
        "    # first time, in init it was called when we CREATED the object\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        # we wanna make sure they are divisible\n",
        "        assert self.d_model % self.nb_proj == 0\n",
        "        # we use 2 slashes to make it integer\n",
        "        self.d_proj = self.d_model // self.nb_proj\n",
        "\n",
        "        self.query_lin = layers.Dense(self.d_model)\n",
        "        self.key_lin = layers.Dense(self.d_model)\n",
        "        self.value_lin = layers.Dense(self.d_model)\n",
        "        self.final_lin = layers.Dense(self.d_model)\n",
        "\n",
        "    def split_proj(self, inputs, batch_size):\n",
        "        \"\"\"\n",
        "        :inputs: [batch_size, seq_len(20), d_model(prev layer dim)]\n",
        "\n",
        "        :return:\n",
        "            dims = [batch_size, nb_proj, seq_len, d_proj]\n",
        "            nb_proj here is like channels in cnn\n",
        "            we basically split the d_model to nb_proj*d_proj so d_proj is\n",
        "            found by doing d_model/nb_proj\n",
        "        \"\"\"\n",
        "        new_shape = (batch_size, -1, self.nb_proj, self.d_proj)\n",
        "        # here we will get: [batch_sz, seq_len, nb_proj, d_proj]\n",
        "\n",
        "        splited_inputs = tf.reshape(inputs, shape=new_shape)\n",
        "\n",
        "        # so we need to reshape it to: [batch_size, nb_proj, seq_len, d_proj]\n",
        "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "\n",
        "    def call(self, queries, keys, values, mask):\n",
        "        batch_size = tf.shape(queries)[0]\n",
        "\n",
        "        queries = self.query_lin(queries)\n",
        "        keys = self.key_lin(keys)\n",
        "        values = self.value_lin(values)\n",
        "\n",
        "        # now we split each of them to make projs\n",
        "        queries = self.split_proj(queries, batch_size)\n",
        "        keys = self.split_proj(keys, batch_size)\n",
        "        values = self.split_proj(values, batch_size)\n",
        "\n",
        "        # each of the q,k,v are [batch_size, nb_proj, seq_len, d_proj]\n",
        "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "\n",
        "        # now we will reverse the splits we did above: reshape + concat\n",
        "        attention = tf.transpose(attention, perm=[0,2,1,3])\n",
        "        # we have [batch_size, seq_len, nb_proj, d_proj] so now we concat 2, 3\n",
        "        concat_attention = tf.reshape(attention, shape=(batch_size, -1, self.d_model))\n",
        "        outputs = self.final_lin(concat_attention)\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N7jK4SvFZEb"
      },
      "source": [
        "## C - Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vEIb2g_5igO"
      },
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "\n",
        "    def __init__(self, FFN_units, nb_proj, dropout):\n",
        "        \"\"\"\n",
        "        :FFN_units:\n",
        "            feed forward networks units: the number of units for the\n",
        "            feed forward which you can see in the encoder part of the\n",
        "            paper (right after the attention there is a feed forward...)\n",
        "        :nb_project:\n",
        "            the number of projections we have (8)\n",
        "        :dropout:\n",
        "            the dropout rate e.g. 0.3\n",
        "        \"\"\"\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout = dropout\n",
        "\n",
        "\n",
        "    # we use this because we dont have many of the vars we want\n",
        "    # when we create the Encoder, so no we can get them when we use the\n",
        "    # function using 'build' instead\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        # we first build the object for the multi-head-attention\n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model, activation=\"relu\")\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "\n",
        "    def call(self, inputs, mask, training):\n",
        "        \"\"\"\n",
        "        :mask: which we will apply in the multi-head attention\n",
        "        :training:\n",
        "            it is true/false which we use dropout while we train=true to stop\n",
        "            the model from overfiting but we dont use it when we are just\n",
        "            testing (aka. train=false)\n",
        "        \"\"\"\n",
        "        # if you look at the architecture you see that in the encoder\n",
        "        # all of the query/key/val are the same array which is the input we\n",
        "        # got from the previous layer\n",
        "        attention = self.multi_head_attention(inputs, inputs, inputs, mask)\n",
        "\n",
        "        # dropout + normalization after the attention\n",
        "        attention = self.dropout_1(attention, training=training)\n",
        "        # we do + inputs here because in the architecture they still concat the\n",
        "        # previous inputs to our resulted attention then we normalize it\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "\n",
        "        # now we do the dense in our FFN:\n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_2(outputs)\n",
        "        outputs = self.norm_2(outputs + attention)\n",
        "\n",
        "        return outputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABusZar2NKT6"
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "                 nb_encoding_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"encoder\"):\n",
        "        # we put name=name here because the name is something that belongs\n",
        "        # to the layers class, so we tell it to use name=\"encoder\"\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "        self.nb_encoding_layers = nb_encoding_layers # the number of encoders in a row\n",
        "        self.d_model = d_model # the size of the output e.g. glove(200)\n",
        "\n",
        "        # we give vocab size for it to know the maximum number used in vocab\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout)\n",
        "        self.enc_layers = [EncoderLayer(FFN_units, nb_proj, dropout)\n",
        "                        for _ in range(self.nb_encoding_layers)]\n",
        "\n",
        "\n",
        "    def call(self, inputs, mask, training):\n",
        "        # look at the paper's architecture while doing these\n",
        "        # embedding with maybe glove weights....\n",
        "        outputs = self.embedding(inputs)\n",
        "        # the reason why we did this was because of what was writtent\n",
        "        # on the paper in secssion 3.4 which they said they multiplied\n",
        "        # it by sqrt of d_model\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        # this will give us the concat: outputs + pos_encoding\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        # now we do dropout before all the encoding layers\n",
        "        # we give it training=bool -> so dont do dropout when training=false\n",
        "        outputs = self.dropout(outputs, training)\n",
        "\n",
        "        # now we do the EmbeddingLayer a couple of times, not just once.\n",
        "        for i in range(self.nb_encoding_layers):\n",
        "            # so we apply it to the (i)th encoder in each for with these params:\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\n",
        "\n",
        "        return outputs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaf9z1oLXgjx"
      },
      "source": [
        "## D - Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXEa6VtkWP4b"
      },
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "\n",
        "    def __init__(self, FFN_units, nb_proj, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "\n",
        "        # MHA 1\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # MHA 2\n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # FFN\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation='relu')\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_3 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        # check the architecture in the paper to see why we do these\n",
        "\n",
        "        # this is the 1# attention\n",
        "        attention = self.multi_head_attention_1(inputs, inputs, inputs, mask_1)\n",
        "        # we give it training=bool -> so dont do dropout when training=false\n",
        "        attention = self.dropout_1(attention, training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "\n",
        "        # this is the 2# attention, this is ALOT different than before one\n",
        "        # pay attention to it's inputs\n",
        "        attention_2 = self.multi_head_attention_2(attention,\n",
        "                                                enc_outputs,\n",
        "                                                enc_outputs,\n",
        "                                                mask_2)\n",
        "        # we give it training=bool -> so dont do dropout when training=false\n",
        "        attention_2 = self.dropout_2(attention_2, training)\n",
        "        attention_2 = self.norm_2(attention_2 + inputs)\n",
        "\n",
        "        # the denses\n",
        "        outputs = self.dense_1(attention_2)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_3(outputs, training)\n",
        "        outputs = self.norm_3(outputs + attention_2)\n",
        "\n",
        "        return outputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLUUmK6hfoj7"
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "                 nb_decoding_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"decoder\"):\n",
        "        super(Decoder, self).__init__(name=name)\n",
        "        self.nb_decoding_layers = nb_decoding_layers # the number of encoders in a row\n",
        "        self.d_model = d_model # the size of the output e.g. glove(200)\n",
        "\n",
        "        # we give vocab size for it to know the maximum number used in vocab\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(FFN_units, nb_proj, dropout)\n",
        "                        for _ in range(nb_decoding_layers)]\n",
        "\n",
        "\n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        # look at the paper's architecture while doing these\n",
        "        # embedding with maybe glove weights....\n",
        "        outputs = self.embedding(inputs)\n",
        "        # the reason why we did this was because of what was writtent\n",
        "        # on the paper in secssion 3.4 which they said they multiplied\n",
        "        # it by sqrt of d_model\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        # this will give us the concat: outputs + pos_encoding\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        # now we do dropout before all the encoding layers\n",
        "        # we give it training=bool -> so dont do dropout when training=false\n",
        "        outputs = self.dropout(outputs, training)\n",
        "\n",
        "        # now we do the EmbeddingLayer a couple of times, not just once.\n",
        "        for i in range(self.nb_decoding_layers):\n",
        "            # so we apply it to the (i)th encoder in each for with these params:\n",
        "            outputs = self.dec_layers[i](outputs,\n",
        "                                             enc_outputs,\n",
        "                                             mask_1,\n",
        "                                             mask_2,\n",
        "                                             training)\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjQz4ZoHhm1v"
      },
      "source": [
        "## E - Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "begalnPMhXVT"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "\n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout,\n",
        "                 name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "\n",
        "        # initing the Objects\n",
        "        self.encoder = Encoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "        self.decoder = Decoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "        # this is at the very end after you combined the enc & dec the output\n",
        "        # will of size vocab_dec\n",
        "        self.last_linear = layers.Dense(units=vocab_size_dec)\n",
        "\n",
        "\n",
        "    def create_padding_mask(self, seq):\n",
        "        \"\"\"\n",
        "        :seq: [batch_size, seq_len(20)]\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # so this gives us element wise equal true/false with broadcasting on 0\n",
        "        # so we just want to see which words dont exist to give it true in all\n",
        "        # the batches\n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "        # so now we will return that mask we made but with 2 broadcasted new\n",
        "        # dimensions so it can match the input needed in attention\n",
        "        # (in the next cell I made and example to see it better)\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    def create_look_ahead_mask(self, seq):\n",
        "        \"\"\"\n",
        "        :seq: [batch_size, seq_len(20)]\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        # sample of what it produces is in the cell below\n",
        "        # why do we do this? because when we predict the ith word we dont see\n",
        "        # words from\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return look_ahead_mask\n",
        "\n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "        # combining the encoder and decoder and masks here\n",
        "\n",
        "        # creating the mask for encoder\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\n",
        "\n",
        "        # creating the mask for decoder\n",
        "\n",
        "        # mask #1 is for the first decoder attention which uses the\n",
        "        # output, output, output as q/k/v so we get max of the 2 masks for it\n",
        "        dec_mask_1 = tf.maximum(self.create_padding_mask(dec_inputs),\n",
        "                                self.create_look_ahead_mask(dec_inputs))\n",
        "        # mask #2 is for the second decoder attention in which we use the\n",
        "        # output of the encoder as v/k so we need to do masking on the input\n",
        "        # so that later when doing q*k then *v we can get a correct output\n",
        "        # this is what the video said, but i belive making this None is alot                  # try making this none later\n",
        "        # more correct since we dont actually use the inputs and outputs but\n",
        "        # their already masked and processed outputs from previous attentions\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "\n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        dec_outputs = self.decoder(dec_inputs,\n",
        "                                   enc_outputs,\n",
        "                                   dec_mask_1,\n",
        "                                   dec_mask_2,\n",
        "                                   training)\n",
        "\n",
        "        outputs = self.last_linear(dec_outputs)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSRSyG4XwOp0"
      },
      "source": [
        "### testing masks to see their outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMj-AkOvn-Tl"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "def create_look_ahead_mask(seq):\n",
        "    seq_len = tf.shape(seq)[1]\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    return look_ahead_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BL6GXKykpln1",
        "outputId": "6dd0c449-71bb-4136-d7f8-4d43e9484a47"
      },
      "source": [
        "# sample: 1 batch of seq_len=8 ->[1,8] this will become [1,1,1,8] then\n",
        "# broadcasting will happen for later stages\n",
        "# mask=1 means we should delete this\n",
        "seq = tf.cast([[1, 2, 3, 0, 4, 0, 0, 0]], tf.int32)\n",
        "\n",
        "print('This padding masking as the name says shows which words exist: (later broadcasting will happen for each batch & nb_proj & d_proj')\n",
        "print(create_padding_mask(seq), end='\\n\\n')\n",
        "\n",
        "print(\"This look ahead masking as the name says shows that only for i>=j we need to keep them (=0's), so we should not see the feature indxes (i<j) (the 1's)\")\n",
        "print('Have in mind that mask=1 means we need to get rid of that, dont confuse it with mask=0')\n",
        "print(1 - tf.linalg.band_part(tf.ones((5, 5)), -1, 0), end='\\n\\n')\n",
        "\n",
        "print('Now applying both: (pay very close attention to this samples output, very important)')\n",
        "print(tf.maximum(create_padding_mask(seq),\n",
        "                 create_look_ahead_mask(seq)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This padding masking as the name says shows which words exist: (later broadcasting will happen for each batch & nb_proj & d_proj\n",
            "tf.Tensor([[[[0. 0. 0. 1. 0. 1. 1. 1.]]]], shape=(1, 1, 1, 8), dtype=float32)\n",
            "\n",
            "This look ahead masking as the name says shows that only for i>=j we need to keep them (=0's), so we should not see the feature indxes (i<j) (the 1's)\n",
            "Have in mind that mask=1 means we need to get rid of that, dont confuse it with mask=0\n",
            "tf.Tensor(\n",
            "[[0. 1. 1. 1. 1.]\n",
            " [0. 0. 1. 1. 1.]\n",
            " [0. 0. 0. 1. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)\n",
            "\n",
            "Now applying both: (pay very close attention to this samples output, very important)\n",
            "tf.Tensor(\n",
            "[[[[0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "   [0. 0. 1. 1. 1. 1. 1. 1.]\n",
            "   [0. 0. 0. 1. 1. 1. 1. 1.]\n",
            "   [0. 0. 0. 1. 1. 1. 1. 1.]\n",
            "   [0. 0. 0. 1. 0. 1. 1. 1.]\n",
            "   [0. 0. 0. 1. 0. 1. 1. 1.]\n",
            "   [0. 0. 0. 1. 0. 1. 1. 1.]\n",
            "   [0. 0. 0. 1. 0. 1. 1. 1.]]]], shape=(1, 1, 8, 8), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzvSJakz0yUJ"
      },
      "source": [
        "# Step 4: Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhfYzD5Oz7cg"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUemDV6BqMTs"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Hyper-parameters:\n",
        "EPOCHS = 2\n",
        "D_MODEL = 128 # 512 takes more time but has a lot better results\n",
        "NB_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT = 0.1 # 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_DE,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout=DROPOUT)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woJ2_wR5kc7h"
      },
      "source": [
        "## Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OJChh-xscqd"
      },
      "source": [
        "# loss\n",
        "\n",
        "# 1) example of SparseCategoricalCrossentropy:\n",
        "# y_true = [1, 2]\n",
        "# y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
        "# 2) we made fromlogits=true. why? so it gives out numbers without softmax\n",
        "# applied to them\n",
        "# 3) we made reduction='none'. why? normally it would sum up all the losses form\n",
        "# all the batches to make it just 1 number but we dont want it, we want to get\n",
        "# rid of the padded losses ourselves then sum it up...\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction='none')\n",
        "def loss_function(target, pred):\n",
        "    # so by using this mask we will get rid of all the losses that\n",
        "    # corrispond to 0 in our target (aka. y_true)\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0)) # [326, 4, 0] -> [1, 1, 0]\n",
        "    loss_ = loss_object(target, pred) # we got the loss numbers (not their softmax probabilities)\n",
        "\n",
        "    # make sure that both loss_ and mask have the same data type so we can mult them\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    # we make the computed losses for 0's 0\n",
        "    loss_ *= mask\n",
        "\n",
        "    # compute the mean loss and return\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "# keeps track of losses during training\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "# keeps track of accs during training\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z_7kqLdkkFB"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL71RRe3kY3Y"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(tf.cast(step, tf.float32))\n",
        "        arg2 = tf.cast(step, tf.float32) * tf.cast((self.warmup_steps**-1.5), tf.float32)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TNz_bWzr1cI"
      },
      "source": [
        "## Checkpoints (delete your checkpoints if there are some unexpected errors when changing your data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9v_ZbpFpxZe"
      },
      "source": [
        "# making a checkpoint:\n",
        "checkpoint_path = \"./ckpt/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# lets check if we already have a checkpoint\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest Checkpoint Restored...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84_3qydkr-4N"
      },
      "source": [
        "## Epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S5Mfu-arnsN",
        "outputId": "fdbe6e43-5dc9-4026-b4aa-0654a4ae13c2"
      },
      "source": [
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    # iterate on each batch:\n",
        "    for (batch_index, (enc_inputs, targets)) in enumerate(dataset):\n",
        "        # we take all the target minus the last word: <s> hello friend <e>. so we get rid of <s>.\n",
        "        # why? because we are trying to predict the next word each time, so at the last step\n",
        "        # we are predicting <e> and we are done, so we wont need it as an input for our decoder\n",
        "        dec_inputs = targets[:, :-1]\n",
        "        # we shift 1 to right because tokens are: <s> hello friend <e>. so we get rid of <s>.\n",
        "        # when we want to do the predictions, we wont need to predict the <s>, we start with <s>\n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "\n",
        "        # this will record everything that happens when we do predictions\n",
        "        with tf.GradientTape() as tape:\n",
        "            # the true is for training\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "\n",
        "        # now we get the gradients using this method using the tape\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        # now we apply the gradients according to our Adam optimizer\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "        # now lets add our loss to the train loss object that keeps track of the loss\n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "\n",
        "        # now let's print our loss and acc from time to time.....\n",
        "        if batch_index % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                epoch+1, batch_index, train_loss.result(), train_accuracy.result()))\n",
        "\n",
        "\n",
        "    # at the end of each epoch we save a checkpoint\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Saved checkpoint for epoch {}!\".format(epoch+1))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start of epoch 1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x79b3915efd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x79b3915efd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 6.2007 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 5.9827 Accuracy 0.0045\n",
            "Epoch 1 Batch 100 Loss 5.9317 Accuracy 0.0272\n",
            "Epoch 1 Batch 150 Loss 5.8887 Accuracy 0.0356\n",
            "Epoch 1 Batch 200 Loss 5.8205 Accuracy 0.0398\n",
            "Epoch 1 Batch 250 Loss 5.7410 Accuracy 0.0426\n",
            "Epoch 1 Batch 300 Loss 5.6381 Accuracy 0.0485\n",
            "Epoch 1 Batch 350 Loss 5.5309 Accuracy 0.0548\n",
            "Epoch 1 Batch 400 Loss 5.4276 Accuracy 0.0597\n",
            "Epoch 1 Batch 450 Loss 5.3287 Accuracy 0.0640\n",
            "Epoch 1 Batch 500 Loss 5.2400 Accuracy 0.0680\n",
            "Epoch 1 Batch 550 Loss 5.1498 Accuracy 0.0723\n",
            "Epoch 1 Batch 600 Loss 5.0622 Accuracy 0.0768\n",
            "Epoch 1 Batch 650 Loss 4.9809 Accuracy 0.0812\n",
            "Epoch 1 Batch 700 Loss 4.9043 Accuracy 0.0856\n",
            "Epoch 1 Batch 750 Loss 4.8297 Accuracy 0.0901\n",
            "Epoch 1 Batch 800 Loss 4.7625 Accuracy 0.0944\n",
            "Epoch 1 Batch 850 Loss 4.6971 Accuracy 0.0987\n",
            "Epoch 1 Batch 900 Loss 4.6312 Accuracy 0.1028\n",
            "Epoch 1 Batch 950 Loss 4.5688 Accuracy 0.1068\n",
            "Epoch 1 Batch 1000 Loss 4.5110 Accuracy 0.1105\n",
            "Epoch 1 Batch 1050 Loss 4.4578 Accuracy 0.1141\n",
            "Epoch 1 Batch 1100 Loss 4.4059 Accuracy 0.1175\n",
            "Epoch 1 Batch 1150 Loss 4.3554 Accuracy 0.1207\n",
            "Epoch 1 Batch 1200 Loss 4.3070 Accuracy 0.1237\n",
            "Epoch 1 Batch 1250 Loss 4.2632 Accuracy 0.1265\n",
            "Epoch 1 Batch 1300 Loss 4.2213 Accuracy 0.1292\n",
            "Epoch 1 Batch 1350 Loss 4.1804 Accuracy 0.1318\n",
            "Epoch 1 Batch 1400 Loss 4.1429 Accuracy 0.1342\n",
            "Epoch 1 Batch 1450 Loss 4.1078 Accuracy 0.1366\n",
            "Epoch 1 Batch 1500 Loss 4.0719 Accuracy 0.1389\n",
            "Epoch 1 Batch 1550 Loss 4.0382 Accuracy 0.1413\n",
            "Epoch 1 Batch 1600 Loss 4.0052 Accuracy 0.1433\n",
            "Epoch 1 Batch 1650 Loss 3.9735 Accuracy 0.1455\n",
            "Epoch 1 Batch 1700 Loss 3.9431 Accuracy 0.1475\n",
            "Epoch 1 Batch 1750 Loss 3.9128 Accuracy 0.1497\n",
            "Epoch 1 Batch 1800 Loss 3.8835 Accuracy 0.1516\n",
            "Epoch 1 Batch 1850 Loss 3.8550 Accuracy 0.1535\n",
            "Epoch 1 Batch 1900 Loss 3.8281 Accuracy 0.1552\n",
            "Epoch 1 Batch 1950 Loss 3.8000 Accuracy 0.1569\n",
            "Epoch 1 Batch 2000 Loss 3.7747 Accuracy 0.1586\n",
            "Epoch 1 Batch 2050 Loss 3.7502 Accuracy 0.1603\n",
            "Epoch 1 Batch 2100 Loss 3.7252 Accuracy 0.1619\n",
            "Epoch 1 Batch 2150 Loss 3.7004 Accuracy 0.1635\n",
            "Epoch 1 Batch 2200 Loss 3.6772 Accuracy 0.1651\n",
            "Epoch 1 Batch 2250 Loss 3.6548 Accuracy 0.1667\n",
            "Epoch 1 Batch 2300 Loss 3.6315 Accuracy 0.1682\n",
            "Epoch 1 Batch 2350 Loss 3.6098 Accuracy 0.1696\n",
            "Epoch 1 Batch 2400 Loss 3.5887 Accuracy 0.1711\n",
            "Epoch 1 Batch 2450 Loss 3.5672 Accuracy 0.1727\n",
            "Epoch 1 Batch 2500 Loss 3.5460 Accuracy 0.1741\n",
            "Epoch 1 Batch 2550 Loss 3.5247 Accuracy 0.1757\n",
            "Epoch 1 Batch 2600 Loss 3.5049 Accuracy 0.1773\n",
            "Epoch 1 Batch 2650 Loss 3.4857 Accuracy 0.1788\n",
            "Epoch 1 Batch 2700 Loss 3.4669 Accuracy 0.1804\n",
            "Epoch 1 Batch 2750 Loss 3.4484 Accuracy 0.1820\n",
            "Epoch 1 Batch 2800 Loss 3.4304 Accuracy 0.1836\n",
            "Epoch 1 Batch 2850 Loss 3.4121 Accuracy 0.1851\n",
            "Epoch 1 Batch 2900 Loss 3.3950 Accuracy 0.1866\n",
            "Epoch 1 Batch 2950 Loss 3.3784 Accuracy 0.1881\n",
            "Epoch 1 Batch 3000 Loss 3.3615 Accuracy 0.1896\n",
            "Epoch 1 Batch 3050 Loss 3.3455 Accuracy 0.1911\n",
            "Epoch 1 Batch 3100 Loss 3.3298 Accuracy 0.1926\n",
            "Epoch 1 Batch 3150 Loss 3.3135 Accuracy 0.1941\n",
            "Saved checkpoint for epoch 1!\n",
            "Start of epoch 2\n",
            "Epoch 2 Batch 0 Loss 2.5523 Accuracy 0.2656\n",
            "Epoch 2 Batch 50 Loss 2.4771 Accuracy 0.2641\n",
            "Epoch 2 Batch 100 Loss 2.4531 Accuracy 0.2683\n",
            "Epoch 2 Batch 150 Loss 2.4316 Accuracy 0.2705\n",
            "Epoch 2 Batch 200 Loss 2.4254 Accuracy 0.2704\n",
            "Epoch 2 Batch 250 Loss 2.4164 Accuracy 0.2717\n",
            "Epoch 2 Batch 300 Loss 2.4047 Accuracy 0.2725\n",
            "Epoch 2 Batch 350 Loss 2.4006 Accuracy 0.2731\n",
            "Epoch 2 Batch 400 Loss 2.3911 Accuracy 0.2743\n",
            "Epoch 2 Batch 450 Loss 2.3831 Accuracy 0.2757\n",
            "Epoch 2 Batch 500 Loss 2.3737 Accuracy 0.2766\n",
            "Epoch 2 Batch 550 Loss 2.3654 Accuracy 0.2778\n",
            "Epoch 2 Batch 600 Loss 2.3591 Accuracy 0.2785\n",
            "Epoch 2 Batch 650 Loss 2.3528 Accuracy 0.2791\n",
            "Epoch 2 Batch 700 Loss 2.3487 Accuracy 0.2796\n",
            "Epoch 2 Batch 750 Loss 2.3431 Accuracy 0.2799\n",
            "Epoch 2 Batch 800 Loss 2.3411 Accuracy 0.2802\n",
            "Epoch 2 Batch 850 Loss 2.3383 Accuracy 0.2807\n",
            "Epoch 2 Batch 900 Loss 2.3327 Accuracy 0.2812\n",
            "Epoch 2 Batch 950 Loss 2.3275 Accuracy 0.2814\n",
            "Epoch 2 Batch 1000 Loss 2.3243 Accuracy 0.2820\n",
            "Epoch 2 Batch 1050 Loss 2.3184 Accuracy 0.2823\n",
            "Epoch 2 Batch 1100 Loss 2.3118 Accuracy 0.2830\n",
            "Epoch 2 Batch 1150 Loss 2.3047 Accuracy 0.2834\n",
            "Epoch 2 Batch 1200 Loss 2.2971 Accuracy 0.2836\n",
            "Epoch 2 Batch 1250 Loss 2.2938 Accuracy 0.2839\n",
            "Epoch 2 Batch 1300 Loss 2.2878 Accuracy 0.2842\n",
            "Epoch 2 Batch 1350 Loss 2.2814 Accuracy 0.2847\n",
            "Epoch 2 Batch 1400 Loss 2.2745 Accuracy 0.2851\n",
            "Epoch 2 Batch 1450 Loss 2.2690 Accuracy 0.2857\n",
            "Epoch 2 Batch 1500 Loss 2.2632 Accuracy 0.2863\n",
            "Epoch 2 Batch 1550 Loss 2.2576 Accuracy 0.2867\n",
            "Epoch 2 Batch 1600 Loss 2.2523 Accuracy 0.2873\n",
            "Epoch 2 Batch 1650 Loss 2.2451 Accuracy 0.2878\n",
            "Epoch 2 Batch 1700 Loss 2.2378 Accuracy 0.2883\n",
            "Epoch 2 Batch 1750 Loss 2.2324 Accuracy 0.2888\n",
            "Epoch 2 Batch 1800 Loss 2.2264 Accuracy 0.2894\n",
            "Epoch 2 Batch 1850 Loss 2.2199 Accuracy 0.2899\n",
            "Epoch 2 Batch 1900 Loss 2.2137 Accuracy 0.2904\n",
            "Epoch 2 Batch 1950 Loss 2.2076 Accuracy 0.2909\n",
            "Epoch 2 Batch 2000 Loss 2.2014 Accuracy 0.2913\n",
            "Epoch 2 Batch 2050 Loss 2.1945 Accuracy 0.2918\n",
            "Epoch 2 Batch 2100 Loss 2.1880 Accuracy 0.2922\n",
            "Epoch 2 Batch 2150 Loss 2.1809 Accuracy 0.2927\n",
            "Epoch 2 Batch 2200 Loss 2.1734 Accuracy 0.2933\n",
            "Epoch 2 Batch 2250 Loss 2.1671 Accuracy 0.2938\n",
            "Epoch 2 Batch 2300 Loss 2.1606 Accuracy 0.2943\n",
            "Epoch 2 Batch 2350 Loss 2.1541 Accuracy 0.2948\n",
            "Epoch 2 Batch 2400 Loss 2.1475 Accuracy 0.2954\n",
            "Epoch 2 Batch 2450 Loss 2.1403 Accuracy 0.2960\n",
            "Epoch 2 Batch 2500 Loss 2.1337 Accuracy 0.2967\n",
            "Epoch 2 Batch 2550 Loss 2.1264 Accuracy 0.2973\n",
            "Epoch 2 Batch 2600 Loss 2.1198 Accuracy 0.2979\n",
            "Epoch 2 Batch 2650 Loss 2.1142 Accuracy 0.2986\n",
            "Epoch 2 Batch 2700 Loss 2.1082 Accuracy 0.2993\n",
            "Epoch 2 Batch 2750 Loss 2.1027 Accuracy 0.3001\n",
            "Epoch 2 Batch 2800 Loss 2.0977 Accuracy 0.3009\n",
            "Epoch 2 Batch 2850 Loss 2.0916 Accuracy 0.3017\n",
            "Epoch 2 Batch 2900 Loss 2.0862 Accuracy 0.3025\n",
            "Epoch 2 Batch 2950 Loss 2.0805 Accuracy 0.3033\n",
            "Epoch 2 Batch 3000 Loss 2.0752 Accuracy 0.3041\n",
            "Epoch 2 Batch 3050 Loss 2.0695 Accuracy 0.3049\n",
            "Epoch 2 Batch 3100 Loss 2.0638 Accuracy 0.3056\n",
            "Epoch 2 Batch 3150 Loss 2.0583 Accuracy 0.3063\n",
            "Saved checkpoint for epoch 2!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DboKBdVg48mc"
      },
      "source": [
        "# Step 5: Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LceVAvXNv7pV"
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "    # turn the sentence to the tokenizer_encoded format [hi, bye] -> [241, 6]\n",
        "    inp_sentence = [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
        "    # expand dim on axis=0 to simulate the batch dimmension\n",
        "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
        "\n",
        "    # let's make the ouput which starts with <s> and add that axis=0 for batch=0\n",
        "    output = tf.expand_dims([VOCAB_SIZE_DE-2], axis=0)\n",
        "\n",
        "    # the loop to predict the next word of output each time and output += it\n",
        "    for _ in range(MAX_LENGTH):\n",
        "        # we put false because we are not training so no dropout\n",
        "        # predictions = [btch_sz=1, seq_len(output_so_far), vocav_sz_de(the\n",
        "        # softmax values of each word, the higher the number the higher the\n",
        "        # probability for that word)]\n",
        "        predictions = transformer(enc_input, output, False)\n",
        "        # we want to take the last word of this prediction\n",
        "        prediction = predictions[:, -1:, :]\n",
        "        # we do argmax to get the index of the most probable next word\n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "\n",
        "        # we reached the end of the sentence\n",
        "        if predicted_id == VOCAB_SIZE_DE-1:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "\n",
        "        # now we know add the new prediction to the last of the output\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    #even if we didn't reach the end of the sentence we can't continue\n",
        "    return tf.squeeze(output, axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxXL5DaPPhKg"
      },
      "source": [
        "def translate(sentence):\n",
        "    output = evaluate(sentence).numpy()\n",
        "    # get rid of <s> and <e> if they exist\n",
        "    output = [i for i in output if i < VOCAB_SIZE_DE-2]\n",
        "    # decode indexes to words e.g. [241, 6] -> [hi, bye]\n",
        "    predicted_sentence = tokenizer_de.decode(output)\n",
        "    print(\"Input: {}\".format(sentence))\n",
        "    print(\"Predicted translation: {}\".format(predicted_sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGOu9PO7Q797",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d568686-9d85-45f2-b3aa-f8e7ad4e1bfe"
      },
      "source": [
        "translate(\"This is a great day!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: This is a great day!\n",
            "Predicted translation: Das ist ein großer Tag für die heutigen Tagung.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# showing This is a big day for today's meeting."
      ],
      "metadata": {
        "id": "lIHY5J8uxPDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"Ready for the exam\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hF32yRpmwUzE",
        "outputId": "444a18d5-0917-4a51-aea9-e3ff639318cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Ready for the exam\n",
            "Predicted translation: Die Rele.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eERcKo3lwYKk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}